{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from utils import extract_and_prepare_data, detrend_ue, detrend_ue_w_quality\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "normalize = True\n",
    "\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    'llama': 'Llama 3.1 8B',\n",
    "    'gemma': 'Gemma 2 9B',\n",
    "    'eurollm': 'EuroLLM 9B',\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'XComet-XCOMET-XXL': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'Comet-wmt22-comet-da': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'bleu_proper': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "}\n",
    "\n",
    "METRICS = {\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': 'MetricX XXL',\n",
    "    'XComet-XCOMET-XXL': 'XComet XXL',\n",
    "    'Comet-wmt22-comet-da': 'Comet WMT22',\n",
    "    # 'bleu_proper': 'BLEU',\n",
    "}\n",
    "\n",
    "pathlib.Path('tables').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('charts').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_header(caption):\n",
    "    return (\n",
    "        \"\\\\begin{table*}\\n\"\n",
    "        \"\\\\footnotesize\\n\"\n",
    "        \"\\\\centering\\n\"\n",
    "        f\"\\caption{{{caption}}}\\n\"\n",
    "        \"\\\\begin{tabular}{lcccccccc}\\n\"\n",
    "        \"\\\\toprule\\n\"\n",
    "        \"&\\multicolumn{4}{c}{\\\\textbf{WMT14}}&\\multicolumn{4}{c}{\\\\textbf{WMT19}}\\\\\\\\\\n\"\n",
    "        \"\\cmidrule(lr){2-5}\\n\"\n",
    "        \"\\cmidrule(lr){6-9}\\n\"\n",
    "\n",
    "    )\n",
    "\n",
    "def footer():\n",
    "    return (\n",
    "        \"\\midrule\\n\"\n",
    "        \"\\end{tabular}\\n\"\n",
    "        \"\\end{table*}\\n\"\n",
    "    )\n",
    "\n",
    "def colname(dataset):\n",
    "    if '_' in dataset:\n",
    "        dataset = dataset.split('_')[1]\n",
    "\n",
    "    return dataset[:2].capitalize() + '-' + dataset[2:].capitalize()\n",
    "\n",
    "\n",
    "def main(type_):\n",
    "    for metric, metric_name in METRICS.items():\n",
    "        datasets = DATASETS[metric]\n",
    "        ue_methods = list(methods_dict.values())\n",
    "\n",
    "        caption = f\"Detailed PRR scores (raw and detrended) for all methods. Metric: {metric_name}.\"\n",
    "        header = get_header(caption)\n",
    "        header += \"&\" + \"&\".join([colname(dataset).replace('_', '\\\\_') for dataset in datasets]) + \"\\\\\\\\\\n\"\n",
    "        latex = header\n",
    "\n",
    "        for model_type in ['base']:\n",
    "            for model, model_name in MODELS.items():\n",
    "                model_title = model_name if model_type == 'base' else f\"{model_name} Instruct\"\n",
    "                latex += \"\\midrule\\n\"\n",
    "                latex += \"& \\\\multicolumn{8}{c}{\" + model_title + \"}\\\\\\\\\\n\"\n",
    "                latex += \"\\midrule\\n\"\n",
    "\n",
    "                ue_scores, _, _ = detrend_ue(datasets, model, model_type, [metric], ue_methods, methods_dict, return_unprocessed=True)\n",
    "\n",
    "                # Find best and second-best scores per dataset (column-wise)\n",
    "                best_map = {}\n",
    "                second_map = {}\n",
    "                for col_idx in range(len(datasets)):\n",
    "                    scores = []\n",
    "                    for method_short in methods_dict.values():\n",
    "                        raw = ue_scores[f\"{method_short}_raw\"][col_idx]\n",
    "                        detr = ue_scores[f\"{method_short}_detr\"][col_idx]\n",
    "                        scores.extend([raw, detr])\n",
    "                    sorted_scores = sorted(scores, reverse=True)\n",
    "                    best_map[col_idx] = sorted_scores[0]\n",
    "                    second_map[col_idx] = sorted_scores[1]\n",
    "\n",
    "                for method_full, method_short in methods_dict.items():\n",
    "                    raw_scores = ue_scores[f\"{method_short}_raw\"]\n",
    "                    detr_scores = ue_scores[f\"{method_short}_detr\"]\n",
    "\n",
    "                    raw_row = [f\"Raw {method_short}\"]\n",
    "                    detr_row = [f\"Detr {method_short}\"]\n",
    "\n",
    "                    for col_idx, (raw, detr) in enumerate(zip(raw_scores, detr_scores)):\n",
    "                        # Format raw\n",
    "                        if np.isclose(raw, best_map[col_idx]):\n",
    "                            raw_row.append(f\"\\\\textbf{{{raw:.2f}}}\")\n",
    "                        elif np.isclose(raw, second_map[col_idx]):\n",
    "                            raw_row.append(f\"\\\\underline{{{raw:.2f}}}\")\n",
    "                        else:\n",
    "                            raw_row.append(f\"{raw:.2f}\")\n",
    "\n",
    "                        # Format detr\n",
    "                        if np.isclose(detr, best_map[col_idx]):\n",
    "                            detr_row.append(f\"\\\\textbf{{{detr:.2f}}}\")\n",
    "                        elif np.isclose(detr, second_map[col_idx]):\n",
    "                            detr_row.append(f\"\\\\underline{{{detr:.2f}}}\")\n",
    "                        else:\n",
    "                            detr_row.append(f\"{detr:.2f}\")\n",
    "\n",
    "                    latex += \" & \".join(raw_row) + \" \\\\\\\\\\n\"\n",
    "                    latex += \" & \".join(detr_row) + \" \\\\\\\\\\n\"\n",
    "                    latex += \"\\\\midrule\\n\"\n",
    "\n",
    "        latex += footer()\n",
    "        name = f'tables/{metric}_detailed_results_per_dataset_ranking_poly_3rd.tex'\n",
    "        with open(name, 'w') as f:\n",
    "            f.write(latex)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    type_='all'\n",
    "    main(type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from utils import extract_and_prepare_data, detrend_ue, detrend_ue_w_quality\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "normalize = True\n",
    "\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    'llama': 'Llama 3.1 8B',\n",
    "    'gemma': 'Gemma 2 9B',\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'XComet-XCOMET-XXL': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'Comet-wmt22-comet-da': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'bleu_proper': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'Accuracy':['gsm8k'],\n",
    "    'AlignScoreInputOutput':['xsum']\n",
    "}\n",
    "\n",
    "METRICS = {\n",
    "    'Accuracy': 'Accuracy',\n",
    "    'AlignScoreInputOutput': 'Align Score',\n",
    "    # 'Comet-wmt22-comet-da': 'Comet WMT22',\n",
    "    # 'bleu_proper': 'BLEU',\n",
    "}\n",
    "\n",
    "pathlib.Path('tables').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('charts').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_header(caption):\n",
    "    return (\n",
    "        \"\\\\begin{table*}\\n\"\n",
    "        \"\\\\footnotesize\\n\"\n",
    "        \"\\\\centering\\n\"\n",
    "        f\"\\caption{{{caption}}}\\n\"\n",
    "        \"\\\\begin{tabular}{lc}\\n\"\n",
    "                \"\\\\toprule\\n\"\n",
    "    )\n",
    "\n",
    "def footer():\n",
    "    return (\n",
    "        \"\\midrule\\n\"\n",
    "        \"\\end{tabular}\\n\"\n",
    "        \"\\end{table*}\\n\"\n",
    "    )\n",
    "\n",
    "def colname(dataset):\n",
    "    if '_' in dataset:\n",
    "        dataset = dataset.split('_')[1]\n",
    "\n",
    "    return dataset[:2].capitalize() + '-' + dataset[2:].capitalize()\n",
    "\n",
    "\n",
    "def main(type_):\n",
    "    for metric, metric_name in METRICS.items():\n",
    "        datasets = DATASETS[metric]\n",
    "        ue_methods = list(methods_dict.values())\n",
    "\n",
    "        caption = f\"Detailed PRR scores (raw and detrended) for all methods. Metric: {metric_name}.\"\n",
    "        header = get_header(caption)\n",
    "        header += \"&\" + \"&\".join([colname(dataset).replace('_', '\\\\_') for dataset in datasets]) + \"\\\\\\\\\\n\"\n",
    "        latex = header\n",
    "\n",
    "        for model_type in ['base']:\n",
    "            for model, model_name in MODELS.items():\n",
    "                model_title = model_name if model_type == 'base' else f\"{model_name} Instruct\"\n",
    "                latex += \"\\midrule\\n\"\n",
    "                latex += \"& \\\\multicolumn{8}{c}{\" + model_title + \"}\\\\\\\\\\n\"\n",
    "                latex += \"\\midrule\\n\"\n",
    "\n",
    "                ue_scores, _ , _, _,_= detrend_ue_w_quality(datasets, model, model_type, [metric], ue_methods, methods_dict, return_unprocessed=True)\n",
    "\n",
    "                # Find best and second-best scores per dataset (column-wise)\n",
    "                best_map = {}\n",
    "                second_map = {}\n",
    "                for col_idx in range(len(datasets)):\n",
    "                    scores = []\n",
    "                    for method_short in methods_dict.values():\n",
    "                        raw = ue_scores[f\"{method_short}_raw\"][col_idx]\n",
    "                        detr = ue_scores[f\"{method_short}_detr\"][col_idx]\n",
    "                        scores.extend([raw, detr])\n",
    "                    sorted_scores = sorted(scores, reverse=True)\n",
    "                    best_map[col_idx] = sorted_scores[0]\n",
    "                    second_map[col_idx] = sorted_scores[1]\n",
    "\n",
    "                for method_full, method_short in methods_dict.items():\n",
    "                    raw_scores = ue_scores[f\"{method_short}_raw\"]\n",
    "                    detr_scores = ue_scores[f\"{method_short}_detr\"]\n",
    "\n",
    "                    raw_row = [f\"Raw {method_short}\"]\n",
    "                    detr_row = [f\"Detr {method_short}\"]\n",
    "\n",
    "                    for col_idx, (raw, detr) in enumerate(zip(raw_scores, detr_scores)):\n",
    "                        # Format raw\n",
    "                        if np.isclose(raw, best_map[col_idx]):\n",
    "                            raw_row.append(f\"\\\\textbf{{{raw:.2f}}}\")\n",
    "                        elif np.isclose(raw, second_map[col_idx]):\n",
    "                            raw_row.append(f\"\\\\underline{{{raw:.2f}}}\")\n",
    "                        else:\n",
    "                            raw_row.append(f\"{raw:.2f}\")\n",
    "\n",
    "                        # Format detr\n",
    "                        if np.isclose(detr, best_map[col_idx]):\n",
    "                            detr_row.append(f\"\\\\textbf{{{detr:.2f}}}\")\n",
    "                        elif np.isclose(detr, second_map[col_idx]):\n",
    "                            detr_row.append(f\"\\\\underline{{{detr:.2f}}}\")\n",
    "                        else:\n",
    "                            detr_row.append(f\"{detr:.2f}\")\n",
    "\n",
    "                    latex += \" & \".join(raw_row) + \" \\\\\\\\\\n\"\n",
    "                    latex += \" & \".join(detr_row) + \" \\\\\\\\\\n\"\n",
    "                    latex += \"\\\\midrule\\n\"\n",
    "\n",
    "        latex += footer()\n",
    "        name = f'tables/{metric}_detailed_results_per_dataset_ranking_w_quality_poly_3d.tex'\n",
    "        with open(name, 'w') as f:\n",
    "            f.write(latex)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    type_='all'\n",
    "    main(type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from utils import extract_and_prepare_data, detrend_ue\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "normalize = True\n",
    "\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    'llama': 'Llama 3.1 8B',\n",
    "    'gemma': 'Gemma 2 9B',\n",
    "    'eurollm': 'EuroLLM 9B',\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'XComet-XCOMET-XXL': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'Comet-wmt22-comet-da': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'bleu_proper': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "}\n",
    "\n",
    "METRICS = {\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': 'MetricX XXL',\n",
    "    'XComet-XCOMET-XXL': 'XComet XXL',\n",
    "    'Comet-wmt22-comet-da': 'Comet WMT22',\n",
    "    # 'bleu_proper': 'BLEU',\n",
    "}\n",
    "\n",
    "pathlib.Path('tables').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('charts').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_header(caption):\n",
    "    return (\n",
    "        \"\\\\begin{table*}\\n\"\n",
    "        \"\\\\footnotesize\\n\"\n",
    "        f\"\\caption{{{caption}}}\\n\"\n",
    "        \"\\\\begin{tabular}{lcccccccc}\\n\"\n",
    "        \"&\\multicolumn{4}{c}{\\\\textbf{WMT14}}&\\multicolumn{4}{c}{\\\\textbf{WMT19}}\\\\\\\\\\n\"\n",
    "        \"\\cmidrule(lr){2-5}\\n\"\n",
    "        \"\\cmidrule(lr){6-9}\\n\"\n",
    "    )\n",
    "\n",
    "def footer():\n",
    "    return (\n",
    "        \"\\midrule\\n\"\n",
    "        \"\\end{tabular}\\n\"\n",
    "        \"\\end{table*}\\n\"\n",
    "    )\n",
    "\n",
    "def colname(dataset):\n",
    "    if '_' in dataset:\n",
    "        dataset = dataset.split('_')[1]\n",
    "\n",
    "    return dataset[:2].capitalize() + '-' + dataset[2:].capitalize()\n",
    "def format_score_pair(raw, detr, threshold=0.00):\n",
    "    arrow = \"$\\\\uparrow$\" if detr - raw > threshold else \"\"\n",
    "    return f\"{raw:.2f}\", f\"{detr:.2f}{arrow}\"\n",
    "\n",
    "def main(type_):\n",
    "    caption = \"Best raw and detrended PRR scores across all metrics and models.\"\n",
    "    latex = \"\\\\begin{table*}[ht]\\n\\\\centering\\n\\\\small\\n\"\n",
    "    latex += f\"\\\\caption{{{caption}}}\\n\"\n",
    "    total_cols = max(len(v) for v in DATASETS.values())\n",
    "    col_spec = \"l\" + \"cc\" * total_cols\n",
    "    latex += f\"\\\\begin{{tabular}}{{{col_spec}}}\\n\\\\toprule\\n\"\n",
    "\n",
    "    # Dataset header (generic, since all blocks will have same layout)\n",
    "    dataset_line = \"Model\"\n",
    "    subheader_line = \" \"\n",
    "    example_metric = next(iter(METRICS))\n",
    "    for d in DATASETS[example_metric]:\n",
    "        name = colname(d)\n",
    "        dataset_line += f\" & \\\\multicolumn{{2}}{{c}}{{{name}}}\"\n",
    "        subheader_line += \" & Raw & Detr\"\n",
    "    latex += dataset_line + \" \\\\\\\\\\n\"\n",
    "    latex += subheader_line + \" \\\\\\\\\\n\\\\midrule\\n\"\n",
    "\n",
    "    for metric, metric_name in METRICS.items():\n",
    "        datasets = DATASETS[metric]\n",
    "        ue_methods = list(methods_dict.values())\n",
    "\n",
    "        # Section header for metric\n",
    "        n_cols = 1 + 2 * len(datasets)\n",
    "        latex += f\"\\\\multicolumn{{{n_cols}}}{{c}}{{\\\\textbf{{{metric_name}}}}} \\\\\\\\\\n\"\n",
    "\n",
    "        for model_key, model_name in MODELS.items():\n",
    "            ue_scores, _, _ = detrend_ue(\n",
    "                datasets, model_key, \"base\", [metric], ue_methods, methods_dict, return_unprocessed=True\n",
    "            )\n",
    "\n",
    "            row = [model_name]\n",
    "            for i in range(len(datasets)):\n",
    "                best_raw = -float(\"inf\")\n",
    "                best_detr = -float(\"inf\")\n",
    "\n",
    "                for method in ue_methods:\n",
    "                    raw = ue_scores[f\"{method}_raw\"][i]\n",
    "                    detr = ue_scores[f\"{method}_detr\"][i]\n",
    "                    best_raw = max(best_raw, raw)\n",
    "                    best_detr = max(best_detr, detr)\n",
    "\n",
    "                raw_fmt, detr_fmt = format_score_pair(best_raw, best_detr)\n",
    "                row.extend([raw_fmt, detr_fmt])\n",
    "\n",
    "            latex += \" & \".join(row) + \" \\\\\\\\\\n\"\n",
    "\n",
    "        latex += \"\\\\midrule\\n\"\n",
    "\n",
    "    latex += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table*}\\n\"\n",
    "\n",
    "    with open(\"tables/final_rowwise_metric_blocks_poly_3rd.tex\", \"w\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    type_='all'\n",
    "    main(type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from utils import extract_and_prepare_data, detrend_ue, detrend_ue_w_quality, detrend_ue_w_quality_only\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import argparse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "normalize = True\n",
    "\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    'llama': 'Llama 3.1 8B',\n",
    "    'gemma': 'Gemma 2 9B',\n",
    "    'eurollm': 'EuroLLM 9B',\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'XComet-XCOMET-XXL': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'Comet-wmt22-comet-da': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'bleu_proper': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'AlignScoreInputOutput' : ['xsum'],\n",
    "        'Accuracy' : ['gsm8k'],\n",
    "            # 'Rouge_rougeL' : ['xsum'],\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "METRICS = {\n",
    "                # 'Rouge_rougeL' : 'Rouge L',\n",
    "    # 'AlignScoreInputOutput': 'Align Score',\n",
    "    'XComet-XCOMET-XXL': 'XComet XXL',\n",
    "    # 'Accuracy':'Acc',\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': 'MetricX-XXL',\n",
    "    'Comet-wmt22-comet-da' :'Comet WMT22',\n",
    "    # 'bleu_proper': 'BLEU',\n",
    "}\n",
    "\n",
    "pathlib.Path('tables').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('charts').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_header(caption):\n",
    "    return (\n",
    "        \"\\\\begin{table*}\\n\"\n",
    "        \"\\\\footnotesize\\n\"\n",
    "        \"\\\\centering\\n\"\n",
    "        \"\\\\toprule\\n\"\n",
    "        f\"\\caption{{{caption}}}\\n\"\n",
    "        \"\\\\begin{tabular}{lcccccccc}\\n\"\n",
    "        \"&\\multicolumn{4}{c}{\\\\textbf{WMT14}}&\\multicolumn{4}{c}{\\\\textbf{WMT19}}\\\\\\\\\\n\"\n",
    "        \"\\cmidrule(lr){2-5}\\n\"\n",
    "        \"\\cmidrule(lr){6-9}\\n\"\n",
    "    )\n",
    "\n",
    "def footer():\n",
    "    return (\n",
    "        \"\\midrule\\n\"\n",
    "        \"\\end{tabular}\\n\"\n",
    "        \"\\end{table*}\\n\"\n",
    "    )\n",
    "\n",
    "def colname(dataset):\n",
    "    if '_' in dataset:\n",
    "        dataset = dataset.split('_')[1]\n",
    "\n",
    "    return dataset[:2].capitalize() + '-' + dataset[2:].capitalize()\n",
    "\n",
    "def plot_raw_vs_detrended(lengths, raw_scores, detr_scores, method_name, save_path=None):\n",
    "    lengths = np.array(lengths).reshape(-1, 1)\n",
    "    raw_scores = np.array(raw_scores)\n",
    "    detr_scores = np.array(detr_scores)\n",
    "\n",
    "    # Fit linear regression models\n",
    "    reg_raw = LinearRegression().fit(lengths, raw_scores)\n",
    "    reg_detr = LinearRegression().fit(lengths, detr_scores)\n",
    "\n",
    "    # Predict\n",
    "    raw_fit = reg_raw.predict(lengths)\n",
    "    detr_fit = reg_detr.predict(lengths)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(lengths, raw_scores, label='Raw Scores', alpha=0.6, color='blue')\n",
    "    plt.plot(lengths, raw_fit, color='blue', linestyle='--', label=f'Raw Fit (slope={reg_raw.coef_[0]:.3f})')\n",
    "\n",
    "    plt.scatter(lengths, detr_scores, label='Detrended Scores', alpha=0.6, color='green')\n",
    "    plt.plot(lengths, detr_fit, color='green', linestyle='--', label=f'Detrended Fit (slope={reg_detr.coef_[0]:.3f})')\n",
    "\n",
    "    plt.title(f'Length Effect for {method_name}')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Uncertainty Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main(type_):\n",
    "    results = []\n",
    "    for metric, metric_name in METRICS.items():\n",
    "        datasets = DATASETS[metric]\n",
    "        ue_methods = list(methods_dict.values())\n",
    "\n",
    "        for model_type in ['base']:\n",
    "            for model, model_name in MODELS.items():\n",
    "                model_title = model_name if model_type == 'base' else f\"{model_name} Instruct\"\n",
    "\n",
    "                ue_scores, _, _ = detrend_ue(datasets, model, model_type, [metric], ue_methods, methods_dict, return_unprocessed=True)\n",
    "\n",
    "                # Find best and second-best scores per dataset (column-wise)\n",
    "                best_map = {}\n",
    "                second_map = {}\n",
    "                for col_idx in range(len(datasets)):\n",
    "                    scores = []\n",
    "                    for method_short in methods_dict.values():\n",
    "                        raw = ue_scores[f\"{method_short}_raw\"][col_idx]\n",
    "                        detr = ue_scores[f\"{method_short}_detr\"][col_idx]\n",
    "                        scores.extend([raw, detr])\n",
    "                        # print( method_short, metric,datasets[col_idx], ':', raw, ' -> ', detr)\n",
    "                        results.append({\n",
    "                            \"Metric\": metric_name,\n",
    "                            \"Dataset\": datasets[col_idx],\n",
    "                            \"Model\": model_title,\n",
    "                            \"Method\": method_short,\n",
    "                            \"ScoreType\": \"Raw\",\n",
    "                            \"Score\": raw\n",
    "                        })\n",
    "\n",
    "                        results.append({\n",
    "                            \"Metric\": metric_name,\n",
    "                            \"Dataset\": datasets[col_idx],\n",
    "                            \"Model\": model_title,\n",
    "                            \"Method\": method_short,\n",
    "                            \"ScoreType\": \"Detrended\",\n",
    "                            \"Score\": detr\n",
    "                        })\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = \"ue_scores_results_wmt3_poly_3rd.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "                    # sorted_scores = sorted(scores, reverse=True)\n",
    "                    # best_map[col_idx] = sorted_scores[0]\n",
    "                    # second_map[col_idx] = sorted_scores[1]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    type_='all'\n",
    "    main(type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from utils import extract_and_prepare_data, detrend_ue, detrend_ue_w_quality, detrend_ue_w_quality_only\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import argparse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "normalize = True\n",
    "\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    'llama': 'Llama 3.1 8B',\n",
    "    'gemma': 'Gemma 2 9B',\n",
    "    # 'eurollm': 'EuroLLM 9B',\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    'metricx-metricx-24-hybrid-xxl-v2p6': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'XComet-XCOMET-XXL': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'Comet-wmt22-comet-da': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'bleu_proper': [\n",
    "        'wmt14_csen',\n",
    "        'wmt14_deen',\n",
    "        'wmt14_ruen',\n",
    "        'wmt14_fren',\n",
    "        'wmt19_deen',\n",
    "        'wmt19_fien',\n",
    "        'wmt19_lten',\n",
    "        'wmt19_ruen',\n",
    "    ],\n",
    "    'AlignScoreInputOutput' : ['xsum'],\n",
    "        'Accuracy' : ['gsm8k'],\n",
    "            # 'Rouge_rougeL' : ['xsum'],\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "METRICS = {\n",
    "                # 'Rouge_rougeL' : 'Rouge L',\n",
    "    'AlignScoreInputOutput': 'Align Score',\n",
    "    # 'XComet-XCOMET-XXL': 'XComet XXL',\n",
    "    'Accuracy':'Acc',\n",
    "    # 'metricx-metricx-24-hybrid-xxl-v2p6': 'MetricX-XXL',\n",
    "    # 'Comet-wmt22-comet-da' :'Comet WMT22',\n",
    "    # 'bleu_proper': 'BLEU',\n",
    "}\n",
    "\n",
    "pathlib.Path('tables').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('charts').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_header(caption):\n",
    "    return (\n",
    "        \"\\\\begin{table*}\\n\"\n",
    "        \"\\\\footnotesize\\n\"\n",
    "        \"\\\\centering\\n\"\n",
    "        \"\\\\toprule\\n\"\n",
    "        f\"\\caption{{{caption}}}\\n\"\n",
    "        \"\\\\begin{tabular}{lcccccccc}\\n\"\n",
    "        \"&\\multicolumn{4}{c}{\\\\textbf{WMT14}}&\\multicolumn{4}{c}{\\\\textbf{WMT19}}\\\\\\\\\\n\"\n",
    "        \"\\cmidrule(lr){2-5}\\n\"\n",
    "        \"\\cmidrule(lr){6-9}\\n\"\n",
    "    )\n",
    "\n",
    "def footer():\n",
    "    return (\n",
    "        \"\\midrule\\n\"\n",
    "        \"\\end{tabular}\\n\"\n",
    "        \"\\end{table*}\\n\"\n",
    "    )\n",
    "\n",
    "def colname(dataset):\n",
    "    if '_' in dataset:\n",
    "        dataset = dataset.split('_')[1]\n",
    "\n",
    "    return dataset[:2].capitalize() + '-' + dataset[2:].capitalize()\n",
    "\n",
    "def plot_raw_vs_detrended(lengths, raw_scores, detr_scores, method_name, save_path=None):\n",
    "    lengths = np.array(lengths).reshape(-1, 1)\n",
    "    raw_scores = np.array(raw_scores)\n",
    "    detr_scores = np.array(detr_scores)\n",
    "\n",
    "    # Fit linear regression models\n",
    "    reg_raw = LinearRegression().fit(lengths, raw_scores)\n",
    "    reg_detr = LinearRegression().fit(lengths, detr_scores)\n",
    "\n",
    "    # Predict\n",
    "    raw_fit = reg_raw.predict(lengths)\n",
    "    detr_fit = reg_detr.predict(lengths)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(lengths, raw_scores, label='Raw Scores', alpha=0.6, color='blue')\n",
    "    plt.plot(lengths, raw_fit, color='blue', linestyle='--', label=f'Raw Fit (slope={reg_raw.coef_[0]:.3f})')\n",
    "\n",
    "    plt.scatter(lengths, detr_scores, label='Detrended Scores', alpha=0.6, color='green')\n",
    "    plt.plot(lengths, detr_fit, color='green', linestyle='--', label=f'Detrended Fit (slope={reg_detr.coef_[0]:.3f})')\n",
    "\n",
    "    plt.title(f'Length Effect for {method_name}')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Uncertainty Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main(type_):\n",
    "    results = []\n",
    "    for metric, metric_name in METRICS.items():\n",
    "        datasets = DATASETS[metric]\n",
    "        ue_methods = list(methods_dict.values())\n",
    "\n",
    "        for model_type in ['base']:\n",
    "            for model, model_name in MODELS.items():\n",
    "                model_title = model_name if model_type == 'base' else f\"{model_name} Instruct\"\n",
    "\n",
    "                ue_scores, _, _,_,_ = detrend_ue_w_quality(datasets, model, model_type, [metric], ue_methods, methods_dict, return_unprocessed=True)\n",
    "\n",
    "                # Find best and second-best scores per dataset (column-wise)\n",
    "                best_map = {}\n",
    "                second_map = {}\n",
    "                for col_idx in range(len(datasets)):\n",
    "                    scores = []\n",
    "                    for method_short in methods_dict.values():\n",
    "                        raw = ue_scores[f\"{method_short}_raw\"][col_idx]\n",
    "                        detr = ue_scores[f\"{method_short}_detr\"][col_idx]\n",
    "                        scores.extend([raw, detr])\n",
    "                        # print( method_short, metric,datasets[col_idx], ':', raw, ' -> ', detr)\n",
    "                        results.append({\n",
    "                            \"Metric\": metric_name,\n",
    "                            \"Dataset\": datasets[col_idx],\n",
    "                            \"Model\": model_title,\n",
    "                            \"Method\": method_short,\n",
    "                            \"ScoreType\": \"Raw\",\n",
    "                            \"Score\": raw\n",
    "                        })\n",
    "\n",
    "                        results.append({\n",
    "                            \"Metric\": metric_name,\n",
    "                            \"Dataset\": datasets[col_idx],\n",
    "                            \"Model\": model_title,\n",
    "                            \"Method\": method_short,\n",
    "                            \"ScoreType\": \"Detrended\",\n",
    "                            \"Score\": detr\n",
    "                        })\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = \"ue_scores_results_gsm8k_xsum_poly_3rd.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "                    # sorted_scores = sorted(scores, reverse=True)\n",
    "                    # best_map[col_idx] = sorted_scores[0]\n",
    "                    # second_map[col_idx] = sorted_scores[1]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    type_='all'\n",
    "    main(type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your actual DataFrame here\n",
    "df = pd.read_csv(\"ue_scores_results_wmt3.csv\")  # Or use your in-memory DataFrame\n",
    "df_2 = pd.read_csv(\"ue_scores_results_gsm8k_xsum.csv\")\n",
    "\n",
    "df = pd.concat([df, df_2]).reset_index(drop=True)\n",
    "\n",
    "# df = df[df['Metric']!='BLEU'].reset_index(drop=True)\n",
    "# Calculate improvement per Method, Metric, Dataset\n",
    "pivot = df.pivot_table(index=[\"Method\", \"Metric\", \"Dataset\",\"Model\"], columns=\"ScoreType\", values=\"Score\").reset_index()\n",
    "pivot[\"Improvement\"] = pivot[\"Detrended\"] - pivot[\"Raw\"]\n",
    "\n",
    "# print(pivot)\n",
    "# Compute average improvement and std per Method and Metric (i.e. task)\n",
    "summary = pivot.groupby([\"Method\", \"Metric\"])[\"Improvement\"].agg(['mean', 'sem']).reset_index()\n",
    "# print(summary)\n",
    "summary[\"formatted\"] = summary.apply(lambda x: f\"{x['mean']:.2f} $\\\\pm$ {x['sem']:.2f}\", axis=1)\n",
    "\n",
    "# summary\n",
    "# Pivot to get desired LaTeX format\n",
    "latex_table = summary.pivot(index=\"Method\", columns=\"Metric\", values=\"formatted\").fillna(\"â€”\")\n",
    "\n",
    "method_order = [\"MSP\", \"PPL\", \"MTE\", \"MCSE\", \"MCNSE\", \"LSRL\"]\n",
    "\n",
    "# Reindex before converting to LaTeX\n",
    "latex_table = latex_table.reindex(method_order)\n",
    "\n",
    "metric_order = [\"Comet WMT22\", \"XComet XXL\", \"MetricX-XXL\", \"Align Score\", \"Acc\"]\n",
    "\n",
    "# Reindex columns after pivot\n",
    "latex_table = latex_table.reindex(columns=metric_order)\n",
    "\n",
    "# Convert to LaTeX\n",
    "latex_code = latex_table.to_latex(escape=False)\n",
    "print(latex_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
