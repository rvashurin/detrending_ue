{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maiya.goloburda/.conda/envs/detrend/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maiya.goloburda/.conda/envs/detrend/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lm_polygraph.ue_metrics.pred_rej_area import PredictionRejectionArea\n",
    "from lm_polygraph.ue_metrics.ue_metric import (\n",
    "    get_random_scores,\n",
    "    normalize_metric,\n",
    ")\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sacrebleu import CHRF, BLEU\n",
    "from lm_polygraph.utils.manager import UEManager\n",
    "\n",
    "from lm_polygraph.ue_metrics import PredictionRejectionArea\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_managers(dataset, model='llama', model_type='base', suff ='test_qe_enriched'):\n",
    "    prefix = '' if model_type == 'base' else '_instruct'\n",
    "\n",
    "    train_manager =  UEManager.load(f'processed_mans/{model}{prefix}_{dataset}_train_{suff}.man') if suff!='' else  UEManager.load(f'mans/{model}{prefix}_{dataset}_train.man') \n",
    "\n",
    "    return train_manager\n",
    "\n",
    "def extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model='llama', model_type='base', suff ='test_qe_enriched'):\n",
    "    train_manager = load_managers(dataset, model, model_type, suff=suff)\n",
    "\n",
    "    full_ue_methods = list(methods_dict.keys())\n",
    "    ue_methods = list(methods_dict.values())\n",
    "\n",
    "    \n",
    "    train_sequences = train_manager.stats['greedy_tokens']\n",
    "    train_texts = train_manager.stats['greedy_texts']\n",
    "    train_targets = train_manager.stats['target_texts']\n",
    "\n",
    "    train_gen_lengths = np.array([len(seq) for seq in train_sequences])\n",
    "    # gen_lengths = np.array([len(seq) for seq in sequences])\n",
    "\n",
    "    # Get train and test values for metrics and UE, remove union of nans\n",
    "    test_nans = []\n",
    "    train_nans = []\n",
    "\n",
    "    train_metric_values = {}\n",
    "    test_metric_values = {}\n",
    "    for metric in all_metrics:\n",
    "        # values = np.array(manager.gen_metrics[('sequence', metric)])\n",
    "        # test_metric_values[metric] = np.array(values)\n",
    "        # test_nans.extend(np.argwhere(np.isnan(values)).flatten())\n",
    "\n",
    "        train_values = np.array(train_manager.gen_metrics[('sequence', metric)])\n",
    "        train_metric_values[metric] = np.array(train_values)\n",
    "        train_nans.extend(np.argwhere(np.isnan(train_values)).flatten())\n",
    "\n",
    "    train_ue_values = {}\n",
    "    # test_ue_values = {}\n",
    "    for i, method in enumerate(full_ue_methods):\n",
    "        train_values = np.array(train_manager.estimations[('sequence', method)])\n",
    "        train_ue_values[ue_methods[i]] = train_values\n",
    "        train_nans.extend(np.argwhere(np.isnan(train_values)).flatten())\n",
    "\n",
    "        # values = np.array(manager.estimations[('sequence', method)])\n",
    "        # test_ue_values[ue_methods[i]] = values\n",
    "        # test_nans.extend(np.argwhere(np.isnan(values)).flatten())\n",
    "\n",
    "    train_nans = np.unique(train_nans).astype(int)\n",
    "    # test_nans = np.unique(test_nans).astype(int)\n",
    "\n",
    "    # Remove nans\n",
    "    for metric in all_metrics:\n",
    "        # test_metric_values[metric] = np.delete(test_metric_values[metric], test_nans)\n",
    "        train_metric_values[metric] = np.delete(train_metric_values[metric], train_nans)\n",
    "\n",
    "    for method in ue_methods:\n",
    "        # test_ue_values[method] = np.delete(test_ue_values[method], test_nans)\n",
    "        train_ue_values[method] = np.delete(train_ue_values[method], train_nans)\n",
    "\n",
    "    train_gen_lengths = np.delete(train_gen_lengths, train_nans)\n",
    "\n",
    "    return train_ue_values, train_metric_values, train_gen_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "}\n",
    "\n",
    "DATASETS = [\n",
    "    'wmt14_deen',\n",
    "    'wmt14_fren',\n",
    "    'wmt14_csen',\n",
    "    'wmt14_ruen',\n",
    "    'wmt19_ruen',\n",
    "    'wmt19_fien',\n",
    "    'wmt19_deen',\n",
    "    'wmt19_lten'\n",
    "]\n",
    "\n",
    "all_metrics = ['Comet-wmt22-comet-da', 'XComet-XCOMET-XXL', 'metricx-metricx-24-hybrid-large-v2p6']\n",
    "all_methods =['MSP', 'PPL', 'MTE', 'MCSE', 'MCNSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4, rc={\"font.family\": \"serif\"})\n",
    "\n",
    "methrics_dict ={\n",
    "  'Comet':'Comet', 'XComet-XCOMET-XXL' :'XComet-XXL', 'metricx-metricx-24-hybrid-large-v2p6' :'MetricX-Large' ,\n",
    "  'AlignScoreInputOutput':'Align Score', 'Accuracy':'Acc', 'AlignScoreInputOutput':'Align Score','Rouge_rougeL':'Rouge L'\n",
    "}\n",
    "\n",
    "def format_dataset_name(raw_name):\n",
    "    try:\n",
    "        prefix, lang_pair = raw_name.split(\"_\")\n",
    "        prefix = prefix.upper()\n",
    "\n",
    "        if len(lang_pair) == 4:  # e.g., fren → Fr-En\n",
    "            src = lang_pair[:2].capitalize()\n",
    "            tgt = lang_pair[2:].capitalize()\n",
    "            lang_fmt = f\"{src}-{tgt}\"\n",
    "        else:\n",
    "            lang_fmt = lang_pair.upper()\n",
    "\n",
    "        return f\"{prefix} {lang_fmt}\"\n",
    "    except Exception:\n",
    "        return raw_name.upper()\n",
    "\n",
    "\n",
    "def plot_metric_vs_length(\n",
    "    gen_lengths, metric_values,\n",
    "    metric_name, dataset_name,\n",
    "    mode='Train', save_path='plot.pdf', model=''\n",
    "):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from scipy.stats import linregress\n",
    "\n",
    "    # Trim outliers\n",
    "    upper_q, lower_q = np.quantile(gen_lengths, [0.95, 0.05])\n",
    "    mask = (gen_lengths > lower_q) & (gen_lengths < upper_q)\n",
    "    gen_lengths = gen_lengths[mask]\n",
    "    metric_values = metric_values[mask]\n",
    "\n",
    "    # Normalize\n",
    "    scaler_len = MinMaxScaler()\n",
    "    scaler_val = MinMaxScaler()\n",
    "\n",
    "    norm_len = scaler_len.fit_transform(gen_lengths[:, None]).squeeze()\n",
    "    norm_val = scaler_val.fit_transform(metric_values[:, None]).squeeze()\n",
    "\n",
    "    # Bin and smooth\n",
    "    df = pd.DataFrame({\"length\": norm_len, \"metric\": norm_val})\n",
    "    grouped = df.groupby(\"length\").agg(['mean', 'sem'])\n",
    "    x_vals = grouped.index.values\n",
    "    y_vals = grouped['metric']['mean'].values\n",
    "    y_errs = grouped['metric']['sem'].values\n",
    "\n",
    "    # Fit regression (on raw normalized data)\n",
    "    linreg = LinearRegression().fit(norm_len[:, None], norm_val)\n",
    "    slope = linreg.coef_[0]\n",
    "\n",
    "    # Also compute p-value\n",
    "    slope_, intercept_, r_val, p_val, std_err = linregress(norm_len, norm_val)\n",
    "\n",
    "    x_line = np.linspace(0, 1, 100)\n",
    "    y_line = linreg.predict(x_line[:, None])\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(x_vals, y_vals, label='AVG metric value', color=\"navy\")\n",
    "    ax.fill_between(x_vals, y_vals - y_errs, y_vals + y_errs, alpha=0.2, color=\"navy\")\n",
    "    ax.plot(x_line, y_line, linestyle='--', color='crimson', label='Regression Line')\n",
    "\n",
    "    ax.text(0.05, 0.95,\n",
    "            f\"Slope: {slope:.2f}\\n$p$-value: {p_val:.3f}\",\n",
    "            transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    pretty_dataset = format_dataset_name(dataset_name)\n",
    "    ax.set_title(f\"{methrics_dict[metric_name]} vs. Length ({pretty_dataset})\", fontsize=14)\n",
    "    ax.set_xlabel(\"Generated sequence length (normalized)\")\n",
    "    ax.set_ylabel(f\"{methrics_dict[metric_name]} (normalized)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric trends plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os \n",
    "models =['llama','gemma','eurollm']\n",
    "for model in models:\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model=model, suff='full_enriched')\n",
    "\n",
    "        for metric in all_metrics:\n",
    "            os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "\n",
    "            plot_metric_vs_length(\n",
    "                gen_lengths=np.array(train_gen_lengths),\n",
    "                metric_values=np.array(train_metric_values[metric]),\n",
    "                metric_name=metric,\n",
    "                dataset_name=dataset,\n",
    "                mode='Train',\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf',\n",
    "                # model = model\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For xsum and gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os \n",
    "DATASETS =['xsum']\n",
    "all_metrics = ['AlignScoreInputOutput']\n",
    "models =['llama','gemma']\n",
    "for model in models:\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model=model, suff='full_enriched')\n",
    "\n",
    "        for metric in all_metrics:\n",
    "            os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "\n",
    "            plot_metric_vs_length(\n",
    "                gen_lengths=np.array(train_gen_lengths),\n",
    "                metric_values=np.array(train_metric_values[metric]),\n",
    "                metric_name=metric,\n",
    "                dataset_name=dataset,\n",
    "                mode='Train',\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf',\n",
    "                # model = model\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os \n",
    "DATASETS =['gsm8k']\n",
    "all_metrics=['Accuracy']\n",
    "models =['llama','gemma']\n",
    "for model in models:\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_hello(dataset, methods_dict, all_metrics, model=model, suff='full_enriched')\n",
    "\n",
    "        for metric in all_metrics:\n",
    "            os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "\n",
    "            plot_metric_vs_length(\n",
    "                gen_lengths=np.array(train_gen_lengths),\n",
    "                metric_values=np.array(train_metric_values[metric]),\n",
    "                metric_name=metric,\n",
    "                dataset_name=dataset,\n",
    "                mode='Train',\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf',\n",
    "                # model = model\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UE metrics trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "        'LexicalSimilarity_rougeL': 'LSRL'\n",
    "}\n",
    "\n",
    "methrics_dict ={\n",
    "  'MSP':'MSP', 'PPL' :'PPL', 'MTE' :'MTE',  'MCSE':'MCSE', 'MCNSE':'MCNSE', 'LSRL': 'LSRL'\n",
    "}\n",
    "\n",
    "DATASETS = [\n",
    "    'wmt14_deen',\n",
    "    'wmt14_fren',\n",
    "    'wmt14_csen',\n",
    "    'wmt14_ruen',\n",
    "    'wmt19_ruen',\n",
    "    'wmt19_fien',\n",
    "    'wmt19_deen',\n",
    "    'wmt19_lten'\n",
    "]\n",
    "\n",
    "all_metrics = ['Comet-wmt22-comet-da', 'XComet-XCOMET-XXL', 'metricx-metricx-24-hybrid-large-v2p6']\n",
    "models = ['llama', 'gemma', 'eurollm']\n",
    "\n",
    "for model in models:\n",
    "    for dataset in DATASETS:\n",
    "        train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model=model, suff='full_enriched')\n",
    "\n",
    "        for metric, metric_short  in methods_dict.items():\n",
    "            os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "            plot_metric_vs_length(\n",
    "                gen_lengths=np.array(train_gen_lengths),\n",
    "                metric_values=np.array(train_ue_values[metric_short]),\n",
    "                metric_name=metric_short,\n",
    "                dataset_name=dataset,\n",
    "                mode='Train',\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf',\n",
    "                # model = model\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detrending results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def format_dataset_name(raw_name):\n",
    "    try:\n",
    "        prefix, lang_pair = raw_name.split(\"_\")\n",
    "        prefix = prefix.upper()\n",
    "\n",
    "        if len(lang_pair) == 4:  # e.g., fren → Fr-En\n",
    "            src = lang_pair[:2].capitalize()\n",
    "            tgt = lang_pair[2:].capitalize()\n",
    "            lang_fmt = f\"{src}-{tgt}\"\n",
    "        else:\n",
    "            lang_fmt = lang_pair.upper()\n",
    "\n",
    "        return f\"{prefix} {lang_fmt}\"\n",
    "    except Exception:\n",
    "        return raw_name.upper()\n",
    "def plot_raw_and_detrended_vs_length(\n",
    "    lengths,\n",
    "    raw_scores,\n",
    "    detr_scores,\n",
    "    metric_name='Metric',\n",
    "    dataset_name='Dataset',\n",
    "    save_path=None\n",
    "):\n",
    "    # Convert to numpy arrays\n",
    "    lengths = np.array(lengths).reshape(-1)\n",
    "    raw_scores = np.array(raw_scores).reshape(-1)\n",
    "    detr_scores = np.array(detr_scores).reshape(-1)\n",
    "\n",
    "    # Remove outliers: keep values between 5th and 95th percentile\n",
    "    lower, upper = np.percentile(lengths, [2.5, 97.5])\n",
    "    mask = (lengths >= lower) & (lengths <= upper)\n",
    "    lengths = lengths[mask]\n",
    "    raw_scores = raw_scores[mask]\n",
    "    detr_scores = detr_scores[mask]\n",
    "\n",
    "    # Fit regressions\n",
    "    reg_raw = LinearRegression().fit(lengths[:, None], raw_scores)\n",
    "    reg_detr = LinearRegression().fit(lengths[:, None], detr_scores)\n",
    "\n",
    "    slope_raw = reg_raw.coef_[0]\n",
    "    slope_detr = reg_detr.coef_[0]\n",
    "\n",
    "    _, _, _, pval_raw, _ = linregress(lengths, raw_scores)\n",
    "    _, _, _, pval_detr, _ = linregress(lengths, detr_scores)\n",
    "\n",
    "    # Prediction lines\n",
    "    x_line = np.linspace(lengths.min(), lengths.max(), 100)\n",
    "    y_raw_line = reg_raw.predict(x_line[:, None])\n",
    "    y_detr_line = reg_detr.predict(x_line[:, None])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(lengths, raw_scores, color='navy', alpha=0.2, label='Raw')\n",
    "    plt.plot(x_line, y_raw_line, '--', color='navy', label=f'Raw Fit (slope={slope_raw:.2f})')\n",
    "\n",
    "    plt.scatter(lengths, detr_scores, color='crimson', alpha=0.2, label='Detrended')\n",
    "    plt.plot(x_line, y_detr_line, '--', color='crimson', label=f'Detr. Fit (slope={slope_detr:.2f})')\n",
    "\n",
    "    plt.title(f\"{metric_name} vs. Length ({format_dataset_name(dataset_name)})\")\n",
    "    plt.xlabel(\"Generated Sequence Length\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "}\n",
    "\n",
    "DATASETS = [\n",
    "    'wmt14_deen',\n",
    "    'wmt14_fren',\n",
    "    'wmt14_csen',\n",
    "    'wmt14_ruen',\n",
    "    'wmt19_ruen',\n",
    "    'wmt19_fien',\n",
    "    'wmt19_deen',\n",
    "    'wmt19_lten'\n",
    "]\n",
    "\n",
    "all_metrics = ['Comet-wmt22-comet-da', 'XComet-XCOMET-XXL', 'metricx-metricx-24-hybrid-large-v2p6']\n",
    "all_methods =['MSP', 'PPL', 'MTE', 'MCSE', 'MCNSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maiya.goloburda/.conda/envs/detrend/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maiya.goloburda/.conda/envs/detrend/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/maiya.goloburda/.conda/envs/detrend/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f45df1d6290>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f45debda4d0>]\n",
      "llama wmt14_deen Below q ids: 1796\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_raw_and_detrended_vs_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m ue_scores, _, _, lengths \u001b[38;5;241m=\u001b[39m detrend_ue([dataset], model, model_type, [metric], ue_methods, methods_dict, return_unprocessed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ue_method \u001b[38;5;129;01min\u001b[39;00m ue_methods:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mplot_raw_and_detrended_vs_length\u001b[49m(lengths, ue_scores[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mue_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_raw_full\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], ue_scores[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mue_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_detr_full\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], metric_name\u001b[38;5;241m=\u001b[39mue_method, dataset_name\u001b[38;5;241m=\u001b[39mdataset, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mue_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_comparison.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_raw_and_detrended_vs_length' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import extract_and_prepare_data, detrend_ue, detrend_ue_w_quality\n",
    "# model = 'llama'\n",
    "# datasets = ['wmt14_deen']\n",
    "model_type ='base'\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "models=['llama', 'gemma', 'eurollm']\n",
    "for metric in all_metrics:\n",
    "    for model in models:\n",
    "        for dataset in DATASETS:\n",
    "            ue_methods =  list(methods_dict.values())\n",
    "            ue_scores, _, _, lengths = detrend_ue([dataset], model, model_type, [metric], ue_methods, methods_dict, return_unprocessed=True)\n",
    "            for ue_method in ue_methods:\n",
    "                plot_raw_and_detrended_vs_length(lengths, ue_scores[f'{ue_method}_raw_full'][0], ue_scores[f'{ue_method}_detr_full'][0], metric_name=ue_method, dataset_name=dataset, save_path=f\"{model}_{ue_method}_{dataset}_comparison.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detrend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
