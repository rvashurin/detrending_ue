{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lm_polygraph.ue_metrics.pred_rej_area import PredictionRejectionArea\n",
    "from lm_polygraph.ue_metrics.ue_metric import (\n",
    "    get_random_scores,\n",
    "    normalize_metric,\n",
    ")\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sacrebleu import CHRF, BLEU\n",
    "from lm_polygraph.utils.manager import UEManager\n",
    "\n",
    "from lm_polygraph.ue_metrics import PredictionRejectionArea\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_managers(dataset, model='llama', model_type='base', suff ='test_qe_enriched'):\n",
    "    if model_type == 'base':\n",
    "        prefix = ''\n",
    "    else:\n",
    "        prefix = '_' + model_type\n",
    "\n",
    "    train_manager =  UEManager.load(f'processed_mans/{model}{prefix}_{dataset}_train_{suff}.man') if suff!='' else  UEManager.load(f'mans/{model}{prefix}_{dataset}_train.man') \n",
    "\n",
    "    return train_manager\n",
    "\n",
    "def extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model='llama', model_type='base', suff ='test_qe_enriched'):\n",
    "    train_manager = load_managers(dataset, model, model_type, suff=suff)\n",
    "\n",
    "    full_ue_methods = list(methods_dict.keys())\n",
    "    ue_methods = list(methods_dict.values())\n",
    "\n",
    "    \n",
    "    train_sequences = train_manager.stats['greedy_tokens']\n",
    "    train_texts = train_manager.stats['greedy_texts']\n",
    "    train_targets = train_manager.stats['target_texts']\n",
    "\n",
    "    train_gen_lengths = np.array([len(seq) for seq in train_sequences])\n",
    "    # gen_lengths = np.array([len(seq) for seq in sequences])\n",
    "\n",
    "    # Get train and test values for metrics and UE, remove union of nans\n",
    "    test_nans = []\n",
    "    train_nans = []\n",
    "\n",
    "    train_metric_values = {}\n",
    "    test_metric_values = {}\n",
    "    for metric in all_metrics:\n",
    "        # values = np.array(manager.gen_metrics[('sequence', metric)])\n",
    "        # test_metric_values[metric] = np.array(values)\n",
    "        # test_nans.extend(np.argwhere(np.isnan(values)).flatten())\n",
    "\n",
    "        train_values = np.array(train_manager.gen_metrics[('sequence', metric)])\n",
    "        train_metric_values[metric] = np.array(train_values)\n",
    "        train_nans.extend(np.argwhere(np.isnan(train_values)).flatten())\n",
    "\n",
    "    train_ue_values = {}\n",
    "    # test_ue_values = {}\n",
    "    for i, method in enumerate(full_ue_methods):\n",
    "        train_values = np.array(train_manager.estimations[('sequence', method)])\n",
    "        train_ue_values[ue_methods[i]] = train_values\n",
    "        train_nans.extend(np.argwhere(np.isnan(train_values)).flatten())\n",
    "\n",
    "        # values = np.array(manager.estimations[('sequence', method)])\n",
    "        # test_ue_values[ue_methods[i]] = values\n",
    "        # test_nans.extend(np.argwhere(np.isnan(values)).flatten())\n",
    "\n",
    "    train_nans = np.unique(train_nans).astype(int)\n",
    "    # test_nans = np.unique(test_nans).astype(int)\n",
    "\n",
    "    # Remove nans\n",
    "    for metric in all_metrics:\n",
    "        # test_metric_values[metric] = np.delete(test_metric_values[metric], test_nans)\n",
    "        train_metric_values[metric] = np.delete(train_metric_values[metric], train_nans)\n",
    "        if len(train_metric_values[metric]) == 0:\n",
    "            set_trace()\n",
    "\n",
    "    for method in ue_methods:\n",
    "        # test_ue_values[method] = np.delete(test_ue_values[method], test_nans)\n",
    "        train_ue_values[method] = np.delete(train_ue_values[method], train_nans)\n",
    "\n",
    "    train_gen_lengths = np.delete(train_gen_lengths, train_nans)\n",
    "\n",
    "    return train_ue_values, train_metric_values, train_gen_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "}\n",
    "\n",
    "DATASETS = [\n",
    "    'wmt14_deen',\n",
    "    'wmt14_fren',\n",
    "    'wmt14_csen',\n",
    "    'wmt14_ruen',\n",
    "    'wmt19_ruen',\n",
    "    'wmt19_fien',\n",
    "    'wmt19_deen',\n",
    "    'wmt19_lten'\n",
    "]\n",
    "\n",
    "all_metrics = ['Comet-wmt22-comet-da', 'XComet-XCOMET-XXL', 'metricx-metricx-24-hybrid-xxl-v2p6']\n",
    "all_methods =['MSP', 'PPL', 'MTE', 'MCSE', 'MCNSE']\n",
    "model_types = ['base', 'instruct', 'instruct_zeroshot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4, rc={\"font.family\": \"serif\"})\n",
    "\n",
    "metrics_dict ={\n",
    "  'Comet-wmt22-comet-da':'Comet', 'XComet-XCOMET-XXL' :'XComet-XXL', 'metricx-metricx-24-hybrid-xxl-v2p6' :'MetricX-XXL' ,\n",
    "  'AlignScoreInputOutput':'Align Score', 'Accuracy':'Acc', 'AlignScoreInputOutput':'Align Score','Rouge_rougeL':'Rouge L'\n",
    "}\n",
    "\n",
    "def format_dataset_name(raw_name):\n",
    "    try:\n",
    "        prefix, lang_pair = raw_name.split(\"_\")\n",
    "        prefix = prefix.upper()\n",
    "\n",
    "        if len(lang_pair) == 4:  # e.g., fren → Fr-En\n",
    "            src = lang_pair[:2].capitalize()\n",
    "            tgt = lang_pair[2:].capitalize()\n",
    "            lang_fmt = f\"{src}-{tgt}\"\n",
    "        else:\n",
    "            lang_fmt = lang_pair.upper()\n",
    "\n",
    "        return f\"{prefix} {lang_fmt}\"\n",
    "    except Exception:\n",
    "        return raw_name.upper()\n",
    "\n",
    "\n",
    "def plot_metric_vs_length(\n",
    "    gen_lengths, metric_values,\n",
    "    metric_name, dataset_name,\n",
    "    mode='Train', save_path='plot.pdf', model='', ax=None, title=None\n",
    "):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from scipy.stats import linregress\n",
    "\n",
    "    # Trim outliers\n",
    "    upper_q, lower_q = np.quantile(gen_lengths, [0.95, 0.05])\n",
    "    mask = (gen_lengths > lower_q) & (gen_lengths < upper_q)\n",
    "    \n",
    "    gen_lengths = gen_lengths[mask]\n",
    "    metric_values = metric_values[mask]    \n",
    "\n",
    "    # Normalize\n",
    "    scaler_len = MinMaxScaler()\n",
    "    scaler_val = MinMaxScaler()\n",
    "\n",
    "    norm_len = scaler_len.fit_transform(gen_lengths[:, None]).squeeze()\n",
    "    norm_val = scaler_val.fit_transform(metric_values[:, None]).squeeze()\n",
    "\n",
    "    # Bin and smooth\n",
    "    df = pd.DataFrame({\"length\": norm_len, \"metric\": norm_val})\n",
    "    grouped = df.groupby(\"length\").agg(['mean', 'sem'])\n",
    "    x_vals = grouped.index.values\n",
    "    y_vals = grouped['metric']['mean'].values\n",
    "    y_errs = grouped['metric']['sem'].values\n",
    "\n",
    "    # Fit regression (on raw normalized data)\n",
    "    linreg = LinearRegression().fit(norm_len[:, None], norm_val)\n",
    "    slope = linreg.coef_[0]\n",
    "\n",
    "    # Also compute p-value\n",
    "    slope_, intercept_, r_val, p_val, std_err = linregress(norm_len, norm_val)\n",
    "\n",
    "    x_line = np.linspace(0, 1, 100)\n",
    "    y_line = linreg.predict(x_line[:, None])\n",
    "\n",
    "    # Plot\n",
    "    if ax is None:\n",
    "        save_plot = True\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    else:\n",
    "        save_plot = False\n",
    "        \n",
    "    ax.plot(x_vals, y_vals, label='AVG metric value', color=\"navy\")\n",
    "    ax.fill_between(x_vals, y_vals - y_errs, y_vals + y_errs, alpha=0.2, color=\"navy\")\n",
    "    ax.plot(x_line, y_line, linestyle='--', color='crimson', label='Regression Line')\n",
    "\n",
    "    ax.text(0.05, 0.95,\n",
    "            f\"Slope: {slope:.2f}\\n$p$-value: {p_val:.3f}\",\n",
    "            transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    pretty_dataset = format_dataset_name(dataset_name)\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=14)\n",
    "    else:\n",
    "        ax.set_title(f\"{metrics_dict[metric_name]} vs. Length ({pretty_dataset})\", fontsize=14)\n",
    "    ax.set_xlabel(\"Generated sequence length (normalized)\")\n",
    "    ax.set_ylabel(f\"{metrics_dict[metric_name]} (normalized)\")\n",
    "    #ax.legend()\n",
    "\n",
    "    if save_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric trends plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os \n",
    "models =['llama','gemma','eurollm']\n",
    "\n",
    "for model in models:\n",
    "    for dataset in DATASETS:\n",
    "\n",
    "        for metric in all_metrics:\n",
    "            fig, axs = plt.subplots(1, len(model_types), figsize=(len(model_types)*9, 9))\n",
    "        \n",
    "            for i, ax in enumerate(axs):\n",
    "                model_type = model_types[i]\n",
    "                \n",
    "                train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model=model, model_type=model_type, suff='full_enriched')\n",
    "                \n",
    "                os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf'\n",
    "                plot_metric_vs_length(\n",
    "                    gen_lengths=np.array(train_gen_lengths),\n",
    "                    metric_values=np.array(train_metric_values[metric]),\n",
    "                    metric_name=metric,\n",
    "                    dataset_name=dataset,\n",
    "                    mode='Train',\n",
    "                    save_path=save_path,\n",
    "                    model = model,\n",
    "                    ax=ax,\n",
    "                    title=model_type\n",
    "                )\n",
    "\n",
    "            pretty_dataset = format_dataset_name(dataset)\n",
    "            fig.suptitle(f\"{metrics_dict[metric]} vs. Length ({pretty_dataset})\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For xsum and gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os \n",
    "DATASETS =['xsum']\n",
    "all_metrics = ['AlignScoreInputOutput']\n",
    "models =['llama','gemma']\n",
    "for model in models:\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model=model, suff='full_enriched')\n",
    "\n",
    "        for metric in all_metrics:\n",
    "            os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "\n",
    "            plot_metric_vs_length(\n",
    "                gen_lengths=np.array(train_gen_lengths),\n",
    "                metric_values=np.array(train_metric_values[metric]),\n",
    "                metric_name=metric,\n",
    "                dataset_name=dataset,\n",
    "                mode='Train',\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf',\n",
    "                # model = model\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os \n",
    "DATASETS =['gsm8k']\n",
    "all_metrics=['Accuracy']\n",
    "models =['llama','gemma']\n",
    "for model in models:\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_hello(dataset, methods_dict, all_metrics, model=model, suff='full_enriched')\n",
    "\n",
    "        for metric in all_metrics:\n",
    "            os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "\n",
    "            plot_metric_vs_length(\n",
    "                gen_lengths=np.array(train_gen_lengths),\n",
    "                metric_values=np.array(train_metric_values[metric]),\n",
    "                metric_name=metric,\n",
    "                    dataset_name=dataset,\n",
    "                mode='Train',\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf',\n",
    "                # model = model\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UE metrics trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "        'LexicalSimilarity_rougeL': 'LSRL'\n",
    "}\n",
    "\n",
    "metrics_dict ={\n",
    "  'MSP':'MSP', 'PPL' :'PPL', 'MTE' :'MTE',  'MCSE':'MCSE', 'MCNSE':'MCNSE', 'LSRL': 'LSRL'\n",
    "}\n",
    "\n",
    "DATASETS = [\n",
    "    'wmt14_deen',\n",
    "    'wmt14_fren',\n",
    "    'wmt14_csen',\n",
    "    'wmt14_ruen',\n",
    "    'wmt19_ruen',\n",
    "    'wmt19_fien',\n",
    "    'wmt19_deen',\n",
    "    'wmt19_lten'\n",
    "]\n",
    "\n",
    "all_metrics = ['Comet-wmt22-comet-da', 'XComet-XCOMET-XXL', 'metricx-metricx-24-hybrid-xxl-v2p6']\n",
    "models = ['llama', 'gemma', 'eurollm']\n",
    "\n",
    "for model in models:\n",
    "    for dataset in DATASETS:\n",
    "        for metric, metric_short  in methods_dict.items():\n",
    "            fig, axs = plt.subplots(1, len(model_types), figsize=(len(model_types)*9, 9))\n",
    "    \n",
    "            for i, ax in enumerate(axs):\n",
    "                model_type = model_types[i]\n",
    "\n",
    "                train_ue_values, train_metric_values, train_gen_lengths = extract_and_prepare_data_train(dataset, methods_dict, all_metrics, model=model, model_type=model_type, suff='full_enriched')\n",
    "\n",
    "                os.makedirs(f'{model}/{metric}', exist_ok=True)\n",
    "                save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_train.pdf'\n",
    "                plot_metric_vs_length(\n",
    "                    gen_lengths=np.array(train_gen_lengths),\n",
    "                    metric_values=np.array(train_ue_values[metric_short]),\n",
    "                    metric_name=metric_short,\n",
    "                    dataset_name=dataset,\n",
    "                    mode='Train',\n",
    "                    save_path=f'{model}/{metric}/{dataset}_{metric}_{model}_{model_types}_train.pdf',\n",
    "                    model = model,\n",
    "                    ax=ax,\n",
    "                    title=model_type\n",
    "                )\n",
    "            pretty_dataset = format_dataset_name(dataset)\n",
    "            fig.suptitle(f\"{methods_dict[metric]} vs. Length ({pretty_dataset})\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detrending results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def format_dataset_name(raw_name):\n",
    "    try:\n",
    "        prefix, lang_pair = raw_name.split(\"_\")\n",
    "        prefix = prefix.upper()\n",
    "\n",
    "        if len(lang_pair) == 4:  # e.g., fren → Fr-En\n",
    "            src = lang_pair[:2].capitalize()\n",
    "            tgt = lang_pair[2:].capitalize()\n",
    "            lang_fmt = f\"{src}-{tgt}\"\n",
    "        else:\n",
    "            lang_fmt = lang_pair.upper()\n",
    "\n",
    "        return f\"{prefix} {lang_fmt}\"\n",
    "    except Exception:\n",
    "        return raw_name.upper()\n",
    "def plot_raw_and_detrended_vs_length(\n",
    "    lengths,\n",
    "    raw_scores,\n",
    "    detr_scores,\n",
    "    metric_name='Metric',\n",
    "    dataset_name='Dataset',\n",
    "    save_path=None\n",
    "):\n",
    "    # Convert to numpy arrays\n",
    "    lengths = np.array(lengths).reshape(-1)\n",
    "    raw_scores = np.array(raw_scores).reshape(-1)\n",
    "    detr_scores = np.array(detr_scores).reshape(-1)\n",
    "\n",
    "    # Remove outliers: keep values between 5th and 95th percentile\n",
    "    lower, upper = np.percentile(lengths, [2.5, 97.5])\n",
    "    mask = (lengths >= lower) & (lengths <= upper)\n",
    "    lengths = lengths[mask]\n",
    "    raw_scores = raw_scores[mask]\n",
    "    detr_scores = detr_scores[mask]\n",
    "\n",
    "    # Fit regressions\n",
    "    reg_raw = LinearRegression().fit(lengths[:, None], raw_scores)\n",
    "    reg_detr = LinearRegression().fit(lengths[:, None], detr_scores)\n",
    "\n",
    "    slope_raw = reg_raw.coef_[0]\n",
    "    slope_detr = reg_detr.coef_[0]\n",
    "\n",
    "    _, _, _, pval_raw, _ = linregress(lengths, raw_scores)\n",
    "    _, _, _, pval_detr, _ = linregress(lengths, detr_scores)\n",
    "\n",
    "    # Prediction lines\n",
    "    x_line = np.linspace(lengths.min(), lengths.max(), 100)\n",
    "    y_raw_line = reg_raw.predict(x_line[:, None])\n",
    "    y_detr_line = reg_detr.predict(x_line[:, None])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(lengths, raw_scores, color='navy', alpha=0.2, label='Raw')\n",
    "    plt.plot(x_line, y_raw_line, '--', color='navy', label=f'Raw Fit (slope={slope_raw:.2f})')\n",
    "\n",
    "    plt.scatter(lengths, detr_scores, color='crimson', alpha=0.2, label='Detrended')\n",
    "    plt.plot(x_line, y_detr_line, '--', color='crimson', label=f'Detr. Fit (slope={slope_detr:.2f})')\n",
    "\n",
    "    plt.title(f\"{metric_name} vs. Length ({format_dataset_name(dataset_name)})\")\n",
    "    plt.xlabel(\"Generated Sequence Length\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "}\n",
    "\n",
    "DATASETS = [\n",
    "    'wmt14_deen',\n",
    "    'wmt14_fren',\n",
    "    'wmt14_csen',\n",
    "    'wmt14_ruen',\n",
    "    'wmt19_ruen',\n",
    "    'wmt19_fien',\n",
    "    'wmt19_deen',\n",
    "    'wmt19_lten'\n",
    "]\n",
    "\n",
    "all_metrics = ['Comet-wmt22-comet-da', 'XComet-XCOMET-XXL', 'metricx-metricx-24-hybrid-large-v2p6']\n",
    "all_methods =['MSP', 'PPL', 'MTE', 'MCSE', 'MCNSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import score_ues, extract_and_prepare_data\n",
    "\n",
    "def detrend_ue(datasets, model, model_type, all_metrics, ue_methods, methods_dict, task='nmt', return_unprocessed=False):\n",
    "    ue_scores = defaultdict(list)\n",
    "    ue_coefs = defaultdict(list)\n",
    "    ave_test_metric_values = {}\n",
    "\n",
    "    if len(all_metrics) == 1 and len(datasets) > 1:\n",
    "        all_metrics = all_metrics * len(datasets)\n",
    "    elif len(all_metrics) != len(datasets):\n",
    "        raise ValueError('Number of metrics and datasets must be the same')\n",
    "\n",
    "    for metric, dataset in zip(all_metrics, datasets):\n",
    "        train_ue_values, \\\n",
    "        test_ue_values, \\\n",
    "        train_metric_values, \\\n",
    "        test_metric_values, \\\n",
    "        train_gen_lengths, \\\n",
    "        gen_lengths = extract_and_prepare_data(dataset, methods_dict, [metric], model=model, model_type=model_type, task=task)\n",
    "\n",
    "        ave_test_metric_values[dataset] = np.mean(test_metric_values[metric])\n",
    "\n",
    "        upper_q = np.quantile(train_gen_lengths, 0.95)\n",
    "        lower_q = np.quantile(train_gen_lengths, 0.05)\n",
    "        below_q_ids = (train_gen_lengths < upper_q) & (train_gen_lengths > lower_q)\n",
    "        print(f'{model} {dataset} Below q ids: {below_q_ids.sum()}')\n",
    "        train_gen_lengths = train_gen_lengths[below_q_ids]\n",
    "\n",
    "        for method in ue_methods:\n",
    "            train_ue_values[method] = train_ue_values[method][below_q_ids]\n",
    "\n",
    "        train_normalized_ue_values = {}\n",
    "        test_normalized_ue_values = {}\n",
    "\n",
    "        ue_residuals = {}\n",
    "\n",
    "        for method in ue_methods:\n",
    "            gen_length_scaler = MinMaxScaler()\n",
    "            train_gen_lengths_normalized = gen_length_scaler.fit_transform(train_gen_lengths[:, np.newaxis]).squeeze()\n",
    "            test_gen_lengths_normalized = gen_length_scaler.transform(gen_lengths[:, np.newaxis]).squeeze()\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            train_normalized_ue_values[method] = scaler.fit_transform(train_ue_values[method][:, np.newaxis]).squeeze()\n",
    "            test_normalized_ue_values[method] = scaler.transform(test_ue_values[method][:, np.newaxis]).squeeze()\n",
    "\n",
    "            linreg = sklearn.linear_model.LinearRegression()\n",
    "            linreg.fit(train_gen_lengths_normalized[:, np.newaxis], train_normalized_ue_values[method])\n",
    "            ue_coefs[method].append(linreg.coef_[0])\n",
    "\n",
    "            ue_residuals[method] = test_normalized_ue_values[method] - linreg.predict(test_gen_lengths_normalized[:, np.newaxis])\n",
    "            scaler = MinMaxScaler()\n",
    "            norm_residuals = scaler.fit_transform(ue_residuals[method][:, np.newaxis]).squeeze()\n",
    "            linreg = sklearn.linear_model.LinearRegression()\n",
    "            linreg.fit(test_gen_lengths_normalized[:, np.newaxis], norm_residuals)\n",
    "            ue_coefs[method].append(linreg.coef_[0])\n",
    "\n",
    "            met_vals = test_metric_values[metric]\n",
    "            raw_score = score_ues(test_ue_values[method], met_vals)\n",
    "            raw_norm_score = score_ues(test_normalized_ue_values[method], met_vals)\n",
    "            detrended_score = score_ues(ue_residuals[method], met_vals)\n",
    "\n",
    "            ue_scores[f'{method}_raw'].append(raw_score)\n",
    "            ue_scores[f'{method}_detr'].append(detrended_score)\n",
    "            ue_scores[f'{method}_raw_full'].append(test_normalized_ue_values[method])\n",
    "            ue_scores[f'{method}_detr_full'].append(ue_residuals[method])\n",
    "\n",
    "\n",
    "    if return_unprocessed:\n",
    "        return ue_scores, ue_coefs, ave_test_metric_values,test_gen_lengths_normalized[:, np.newaxis]\n",
    "\n",
    "    raw_column_values = []\n",
    "    detr_column_values = []\n",
    "    for _id, _ in enumerate(datasets):\n",
    "        raw_column_values.append([ue_scores[f'{method}_raw'][_id] for method in ue_methods])\n",
    "        detr_column_values.append([ue_scores[f'{method}_detr'][_id] for method in ue_methods])\n",
    "\n",
    "        metric_raw_scores = np.array([ue_scores[f'{method}_raw'][_id] for method in ue_methods])\n",
    "        metric_detr_scores = np.array([ue_scores[f'{method}_detr'][_id] for method in ue_methods])\n",
    "\n",
    "        top_raw_id = np.argmax(metric_raw_scores)\n",
    "        top_detr_id = np.argmax(metric_detr_scores)\n",
    "\n",
    "        for method in ue_methods:\n",
    "            ue_scores[f'{method}_raw'][_id] = f'{ue_scores[f\"{method}_raw\"][_id]:.2f}'\n",
    "            ue_scores[f'{method}_detr'][_id] = f'{ue_scores[f\"{method}_detr\"][_id]:.2f}'\n",
    "\n",
    "        # wrap best detr method in bold\n",
    "        ue_scores[f'{ue_methods[top_detr_id]}_detr'][_id] = f'\\\\textbf{{{ue_scores[f\"{ue_methods[top_detr_id]}_detr\"][_id]}}}'\n",
    "        # wrap best raw method in underline\n",
    "        ue_scores[f'{ue_methods[top_raw_id]}_raw'][_id] = f'\\\\underline{{{ue_scores[f\"{ue_methods[top_raw_id]}_raw\"][_id]}}}'\n",
    "\n",
    "    total_column_values = []\n",
    "    for raw_column, detr_column in zip(raw_column_values, detr_column_values):\n",
    "        total_column_values.append([val for pair in zip(raw_column, detr_column) for val in pair])\n",
    "\n",
    "    raw_method_id_ranks = np.flip(np.argsort(raw_column_values, axis=-1), axis=-1)\n",
    "    raw_mean_ranks = [np.nonzero(raw_method_id_ranks == method_i)[1].mean() for method_i, _ in enumerate(ue_methods)]\n",
    "\n",
    "    detr_method_id_ranks = np.flip(np.argsort(detr_column_values, axis=-1), axis=-1)\n",
    "    detr_mean_ranks = [np.nonzero(detr_method_id_ranks == method_i)[1].mean() for method_i, _ in enumerate(ue_methods)]\n",
    "\n",
    "    total_method_id_ranks = np.flip(np.argsort(total_column_values, axis=-1), axis=-1)\n",
    "    total_mean_ranks = [np.nonzero(total_method_id_ranks == method_i)[1].mean() for method_i, _ in enumerate(ue_methods * 2)]\n",
    "\n",
    "    for method_i, method in enumerate(ue_methods):\n",
    "        ue_scores[f'{method}_raw'].extend((str(raw_mean_ranks[method_i]), '-', total_mean_ranks[method_i * 2]))\n",
    "        ue_scores[f'{method}_detr'].extend(('-', str(detr_mean_ranks[method_i]), total_mean_ranks[method_i * 2 + 1]))\n",
    "    \n",
    "    return ue_scores, ue_coefs, ave_test_metric_values\n",
    "\n",
    "\n",
    "\n",
    "def detrend_ue_w_quality(datasets, model, model_type, all_metrics, ue_methods, methods_dict, task='nmt', return_unprocessed=False, quality_fit_sample_size=None):\n",
    "    ue_scores = defaultdict(list)\n",
    "    ue_scores_full = {}\n",
    "    ue_coefs = defaultdict(list)\n",
    "    ave_test_metric_values = {}\n",
    "\n",
    "    if len(all_metrics) == 1 and len(datasets) > 1:\n",
    "        all_metrics = all_metrics * len(datasets)\n",
    "    elif len(all_metrics) != len(datasets):\n",
    "        raise ValueError('Number of metrics and datasets must be the same')\n",
    "\n",
    "    for metric, dataset in zip(all_metrics, datasets):\n",
    "        train_ue_values, \\\n",
    "        test_ue_values, \\\n",
    "        train_metric_values, \\\n",
    "        test_metric_values, \\\n",
    "        train_gen_lengths, \\\n",
    "        gen_lengths = extract_and_prepare_data(dataset, methods_dict, [metric], model=model, model_type=model_type, task=task)\n",
    "\n",
    "        ave_test_metric_values[dataset] = np.mean(test_metric_values[metric])\n",
    "\n",
    "        upper_q = np.quantile(train_gen_lengths, 0.95)\n",
    "        lower_q = np.quantile(train_gen_lengths, 0.05)\n",
    "        below_q_ids = (train_gen_lengths < upper_q) & (train_gen_lengths > lower_q)\n",
    "        print(f'{model} {dataset} Below q ids: {below_q_ids.sum()}')\n",
    "        train_gen_lengths = train_gen_lengths[below_q_ids]\n",
    "\n",
    "        for method in ue_methods:\n",
    "            train_ue_values[method] = train_ue_values[method][below_q_ids]\n",
    "\n",
    "\n",
    "        train_normalized_ue_values = {}\n",
    "        test_normalized_ue_values = {}\n",
    "\n",
    "        train_normalized_metric_values = {}\n",
    "        test_normalized_metric_values = {}\n",
    "        ue_residuals = {}\n",
    "       \n",
    "        # for metric in all_metrics:\n",
    "        #     train_metric_values[metric] = train_metric_values[metric][below_q_ids]\n",
    "\n",
    "\n",
    "        for method in ue_methods:\n",
    "            gen_length_scaler = MinMaxScaler()\n",
    "            train_gen_lengths_normalized = gen_length_scaler.fit_transform(train_gen_lengths[:, np.newaxis]).squeeze()\n",
    "            test_gen_lengths_normalized = gen_length_scaler.transform(gen_lengths[:, np.newaxis]).squeeze()\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            train_normalized_ue_values[method] = scaler.fit_transform(train_ue_values[method][:, np.newaxis]).squeeze()\n",
    "            test_normalized_ue_values[method] = scaler.transform(test_ue_values[method][:, np.newaxis]).squeeze()\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            train_normalized_metric_values[method] = scaler.fit_transform(train_metric_values[metric][:, np.newaxis]).squeeze()\n",
    "            test_normalized_metric_values[method] = scaler.transform(test_metric_values[metric][:, np.newaxis]).squeeze()\n",
    "\n",
    "            # quality_reg = sklearn.linear_model.LinearRegression()\n",
    "            # quality_reg.fit(train_gen_lengths_normalized[:, np.newaxis], train_normalized_metric_values[method][below_q_ids])\n",
    "            # quality_slope = quality_reg.coef_[0]\n",
    "\n",
    "            if quality_fit_sample_size is not None and quality_fit_sample_size < len(train_gen_lengths):\n",
    "                sample_indices = np.random.choice(len(train_gen_lengths), size=quality_fit_sample_size, replace=False)\n",
    "                quality_reg = sklearn.linear_model.LinearRegression()\n",
    "                quality_reg.fit(\n",
    "                    train_gen_lengths_normalized[sample_indices, np.newaxis],\n",
    "                    train_normalized_metric_values[method][sample_indices]\n",
    "                )\n",
    "            else:\n",
    "                quality_reg = sklearn.linear_model.LinearRegression()\n",
    "                quality_reg.fit(\n",
    "                    train_gen_lengths_normalized[:, np.newaxis],\n",
    "                    train_normalized_metric_values[method][below_q_ids]\n",
    "                )\n",
    "\n",
    "            # Fit UE ~ length\n",
    "            linreg = sklearn.linear_model.LinearRegression()\n",
    "            linreg.fit(train_gen_lengths_normalized[:, np.newaxis], train_normalized_ue_values[method])\n",
    "            ue_slope = linreg.coef_[0]\n",
    "            ue_coefs[method].append(ue_slope)\n",
    "\n",
    "            predicted_quality_trend = quality_reg.predict(test_gen_lengths_normalized[:, np.newaxis])\n",
    "            # Predict UE trend on test\n",
    "            predicted_ue_trend = linreg.predict(test_gen_lengths_normalized[:, np.newaxis])\n",
    "\n",
    "\n",
    "            # joint_reg = sklearn.linear_model.LinearRegression()\n",
    "            # joint_reg.fit(np.stack([train_gen_lengths_normalized, train_normalized_metric_values[method]], axis=1),\n",
    "                        # train_normalized_ue_values[method])\n",
    "            # predicted_trend = joint_reg.predict(np.stack([test_gen_lengths_normalized, test_normalized_metric_values[method]], axis=1))\n",
    "            # adjusted_ue = test_normalized_ue_values[method] - predicted_trend\n",
    "\n",
    "            # Residual-based adjustment\n",
    "            adjusted_ue = test_normalized_ue_values[method] - predicted_ue_trend - predicted_quality_trend\n",
    "\n",
    "            residual_reg = sklearn.linear_model.LinearRegression()\n",
    "            residual_reg.fit(test_gen_lengths_normalized[:, np.newaxis], adjusted_ue)\n",
    "            ue_coefs[method].append(residual_reg.coef_[0])\n",
    "\n",
    "            met_vals = test_metric_values[metric]\n",
    "            raw_score = score_ues(test_ue_values[method], met_vals)\n",
    "            raw_norm_score = score_ues(test_normalized_ue_values[method], met_vals)\n",
    "            detrended_score = score_ues(adjusted_ue, met_vals)\n",
    "            ue_scores_full[f'{method}_raw'] = test_normalized_ue_values[method]\n",
    "            ue_scores_full[f'{method}_detr'] = adjusted_ue\n",
    "            ue_scores[f'{method}_raw'].append(raw_score)\n",
    "            ue_scores[f'{method}_detr'].append(detrended_score)\n",
    "            ue_scores[f'{method}_raw_full'].append(test_normalized_ue_values[method])\n",
    "            ue_scores[f'{method}_detr_full'].append(adjusted_ue)\n",
    "\n",
    "\n",
    "\n",
    "    normalized_lengths= test_gen_lengths_normalized.tolist()\n",
    "\n",
    "    if return_unprocessed:\n",
    "        return ue_scores, ue_coefs, ave_test_metric_values,  test_gen_lengths_normalized[:, np.newaxis] , test_normalized_metric_values[method]\n",
    "\n",
    "    raw_column_values = []\n",
    "    detr_column_values = []\n",
    "    for _id, _ in enumerate(datasets):\n",
    "        raw_column_values.append([ue_scores[f'{method}_raw'][_id] for method in ue_methods])\n",
    "        detr_column_values.append([ue_scores[f'{method}_detr'][_id] for method in ue_methods])\n",
    "\n",
    "        metric_raw_scores = np.array([ue_scores[f'{method}_raw'][_id] for method in ue_methods])\n",
    "        metric_detr_scores = np.array([ue_scores[f'{method}_detr'][_id] for method in ue_methods])\n",
    "\n",
    "        top_raw_id = np.argmax(metric_raw_scores)\n",
    "        top_detr_id = np.argmax(metric_detr_scores)\n",
    "\n",
    "        for method in ue_methods:\n",
    "            ue_scores[f'{method}_raw'][_id] = f'{ue_scores[f\"{method}_raw\"][_id]:.2f}'\n",
    "            ue_scores[f'{method}_detr'][_id] = f'{ue_scores[f\"{method}_detr\"][_id]:.2f}'\n",
    "\n",
    "        # wrap best detr method in bold\n",
    "        ue_scores[f'{ue_methods[top_detr_id]}_detr'][_id] = f'\\\\textbf{{{ue_scores[f\"{ue_methods[top_detr_id]}_detr\"][_id]}}}'\n",
    "        # wrap best raw method in underline\n",
    "        ue_scores[f'{ue_methods[top_raw_id]}_raw'][_id] = f'\\\\underline{{{ue_scores[f\"{ue_methods[top_raw_id]}_raw\"][_id]}}}'\n",
    "\n",
    "    total_column_values = []\n",
    "    for raw_column, detr_column in zip(raw_column_values, detr_column_values):\n",
    "        total_column_values.append([val for pair in zip(raw_column, detr_column) for val in pair])\n",
    "\n",
    "    raw_method_id_ranks = np.flip(np.argsort(raw_column_values, axis=-1), axis=-1)\n",
    "    raw_mean_ranks = [np.nonzero(raw_method_id_ranks == method_i)[1].mean() for method_i, _ in enumerate(ue_methods)]\n",
    "\n",
    "    detr_method_id_ranks = np.flip(np.argsort(detr_column_values, axis=-1), axis=-1)\n",
    "    detr_mean_ranks = [np.nonzero(detr_method_id_ranks == method_i)[1].mean() for method_i, _ in enumerate(ue_methods)]\n",
    "\n",
    "    total_method_id_ranks = np.flip(np.argsort(total_column_values, axis=-1), axis=-1)\n",
    "    total_mean_ranks = [np.nonzero(total_method_id_ranks == method_i)[1].mean() for method_i, _ in enumerate(ue_methods * 2)]\n",
    "\n",
    "    for method_i, method in enumerate(ue_methods):\n",
    "        ue_scores[f'{method}_raw'].extend((str(raw_mean_ranks[method_i]), '-', total_mean_ranks[method_i * 2]))\n",
    "        ue_scores[f'{method}_detr'].extend(('-', str(detr_mean_ranks[method_i]), total_mean_ranks[method_i * 2 + 1]))\n",
    "    \n",
    "    return ue_scores, ue_coefs, ave_test_metric_values, normalized_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'llama'\n",
    "DATASETS = ['wmt14_deen']\n",
    "model_type ='base'\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "models=['llama' ] #, 'gemma', 'eurollm']\n",
    "for metric in all_metrics:\n",
    "    for model in models:\n",
    "        for dataset in DATASETS:\n",
    "            ue_methods =  list(methods_dict.values())\n",
    "            ue_scores, _, _, lengths = detrend_ue([dataset], model, model_type, [metric], ue_methods, methods_dict, return_unprocessed=True)\n",
    "            for ue_method in ue_methods:\n",
    "                plot_raw_and_detrended_vs_length(lengths, ue_scores[f'{ue_method}_raw_full'][0], ue_scores[f'{ue_method}_detr_full'][0], metric_name=ue_method, dataset_name=dataset, save_path=f\"{model}_{ue_method}_{dataset}_comparison.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def format_dataset_name(raw_name):\n",
    "    try:\n",
    "        prefix, lang_pair = raw_name.split(\"_\")\n",
    "        prefix = prefix.upper()\n",
    "\n",
    "        if len(lang_pair) == 4:  # e.g., fren → Fr-En\n",
    "            src = lang_pair[:2].capitalize()\n",
    "            tgt = lang_pair[2:].capitalize()\n",
    "            lang_fmt = f\"{src}-{tgt}\"\n",
    "        else:\n",
    "            lang_fmt = lang_pair.upper()\n",
    "\n",
    "        return f\"{prefix} {lang_fmt}\"\n",
    "    except Exception:\n",
    "        return raw_name.upper()\n",
    "def plot_raw_and_detrended_vs_length(\n",
    "    lengths,\n",
    "    raw_scores,\n",
    "    detr_scores,\n",
    "    metric_name='Metric',\n",
    "    dataset_name='Dataset',\n",
    "    save_path=None,\n",
    "    quality_scores=None\n",
    "):\n",
    "    # Convert to numpy arrays\n",
    "    lengths = np.array(lengths).reshape(-1)\n",
    "    raw_scores = np.array(raw_scores).reshape(-1)\n",
    "    detr_scores = np.array(detr_scores).reshape(-1)\n",
    "\n",
    "    # Remove outliers: keep values between 5th and 95th percentile\n",
    "    lower, upper = np.percentile(lengths, [2.5, 97.5])\n",
    "    mask = (lengths >= lower) & (lengths <= upper)\n",
    "    lengths = lengths[mask]\n",
    "    raw_scores = raw_scores[mask]\n",
    "    detr_scores = detr_scores[mask]\n",
    "    quality_scores=quality_scores[mask]\n",
    "\n",
    "    # Fit regressions\n",
    "    reg_raw = LinearRegression().fit(lengths[:, None], raw_scores)\n",
    "    reg_detr = LinearRegression().fit(lengths[:, None], detr_scores)\n",
    "    slope_raw = reg_raw.coef_[0]\n",
    "    slope_detr = reg_detr.coef_[0]\n",
    "\n",
    "    _, _, _, pval_raw, _ = linregress(lengths, raw_scores)\n",
    "    _, _, _, pval_detr, _ = linregress(lengths, detr_scores)\n",
    "\n",
    "    # Prediction lines\n",
    "    x_line = np.linspace(lengths.min(), lengths.max(), 100)\n",
    "    y_raw_line = reg_raw.predict(x_line[:, None])\n",
    "    y_detr_line = reg_detr.predict(x_line[:, None])\n",
    "\n",
    "    if quality_scores[0] is not None:\n",
    "        for i in range(len(quality_scores)):\n",
    "            quality_scores[i] = -quality_scores[i]\n",
    "        reg_qual = LinearRegression().fit(lengths[:, None], quality_scores)\n",
    "        y_quality_line =reg_qual.predict(x_line[:, None])\n",
    "        slope_qual = reg_qual.coef_[0]\n",
    "        _, _, _, pval_qual, _ = linregress(lengths, quality_scores)\n",
    "\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(lengths, raw_scores, color='navy', alpha=0.2, label='Raw')\n",
    "    plt.plot(x_line, y_raw_line, '--', color='navy', label=f'Raw Fit (slope={slope_raw:.2f})')\n",
    "\n",
    "    plt.scatter(lengths, detr_scores, color='crimson', alpha=0.2, label='Detrended')\n",
    "    plt.plot(x_line, y_detr_line, '--', color='crimson', label=f'Detr. Fit (slope={slope_detr:.2f})')\n",
    "\n",
    "    if quality_scores[0] is not None:\n",
    "        plt.scatter(lengths, quality_scores, color='gold', alpha=0.2, label='Quality Scores')\n",
    "        plt.plot(x_line, y_quality_line, '--', color='gold', label=f'Quality. Fit (slope={slope_qual:.2f})')\n",
    "\n",
    "    plt.title(f\"{metric_name} vs. Length ({format_dataset_name(dataset_name)})\")\n",
    "    plt.xlabel(\"Generated Sequence Length\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "model_type ='base'\n",
    "# metric = 'Comet-wmt22-comet-da'\n",
    "methods_dict = {\n",
    "    'MaximumSequenceProbability': 'MSP',\n",
    "    'Perplexity': 'PPL',\n",
    "    'MeanTokenEntropy': 'MTE',\n",
    "    'MonteCarloSequenceEntropy': 'MCSE',\n",
    "    'MonteCarloNormalizedSequenceEntropy': 'MCNSE',\n",
    "    'LexicalSimilarity_rougeL': 'LSRL',\n",
    "}\n",
    "models=['llama', 'gemma']\n",
    "DATASETS = ['xsum', 'gsm8k']\n",
    "for model in models:\n",
    "    for dataset in DATASETS:\n",
    "        metric ='AlignScoreInputOutput' if dataset=='xsum' else 'Accuracy'\n",
    "        ue_methods =  list(methods_dict.values())\n",
    "        ue_scores, _, _, lengths, quality = detrend_ue_w_quality([dataset], model, model_type, [metric], ue_methods, methods_dict, return_unprocessed=True)\n",
    "        for ue_method in ue_methods:\n",
    "            plot_raw_and_detrended_vs_length(lengths, ue_scores[f'{ue_method}_raw_full'][0], ue_scores[f'{ue_method}_detr_full'][0], metric_name=ue_method, dataset_name=dataset, save_path=f\"{model}_{ue_method}_{dataset}_comparison.pdf\", quality_scores=quality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
